<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Enhancing LLM Inference Efficiency with Key-Value Retrieval</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #fff;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 40px 20px;
        }

        header {
            text-align: center;
            margin-bottom: 60px;
            padding-bottom: 30px;
            border-bottom: 1px solid #e1e4e8;
        }

        h1 {
            font-size: 2.5rem;
            font-weight: 600;
            color: #24292e;
            margin-bottom: 20px;
            line-height: 1.2;
        }

        .authors {
            font-size: 1.1rem;
            color: #586069;
            margin-bottom: 8px;
        }

        .affiliation {
            font-size: 1rem;
            color: #6a737d;
        }

        .links {
            margin-top: 25px;
        }

        .btn {
            display: inline-block;
            padding: 8px 16px;
            margin: 0 8px;
            background-color: #0366d6;
            color: white;
            text-decoration: none;
            border-radius: 6px;
            font-weight: 500;
            transition: background-color 0.2s;
        }

        .btn:hover {
            background-color: #0256cc;
        }

        .btn-secondary {
            background-color: #f6f8fa;
            color: #24292e;
            border: 1px solid #e1e4e8;
        }

        .btn-secondary:hover {
            background-color: #f3f4f6;
        }

        section {
            margin-bottom: 40px;
        }

        h2 {
            font-size: 1.8rem;
            font-weight: 600;
            color: #24292e;
            margin-bottom: 20px;
            padding-bottom: 8px;
            border-bottom: 2px solid #fd7e14;
        }

        h3 {
            font-size: 1.3rem;
            font-weight: 600;
            color: #24292e;
            margin-bottom: 15px;
            margin-top: 25px;
        }

        p {
            margin-bottom: 16px;
            text-align: justify;
            font-size: 1rem;
        }

        .abstract {
            background-color: #f6f8fa;
            padding: 20px;
            border-radius: 8px;
            border-left: 4px solid #0366d6;
        }

        .figure {
            text-align: center;
            margin: 30px 0;
        }

        .figure img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }

        .figure-caption {
            font-size: 0.9rem;
            color: #6a737d;
            margin-top: 8px;
            font-style: italic;
        }

        .code-block {
            background-color: #f6f8fa;
            border: 1px solid #e1e4e8;
            border-radius: 6px;
            padding: 16px;
            font-family: 'SF Mono', Monaco, Inconsolata, 'Roboto Mono', Consolas, monospace;
            font-size: 0.9rem;
            overflow-x: auto;
            margin: 20px 0;
        }

        .results-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }

        .result-card {
            background-color: #f6f8fa;
            padding: 20px;
            border-radius: 8px;
            border: 1px solid #e1e4e8;
        }

        .metric {
            font-size: 1.5rem;
            font-weight: 600;
            color: #0366d6;
        }

        .metric-label {
            font-size: 0.9rem;
            color: #6a737d;
            margin-top: 4px;
        }

        ul {
            margin-left: 20px;
            margin-bottom: 16px;
        }

        li {
            margin-bottom: 8px;
        }

        .citation {
            background-color: #fff5b4;
            border: 1px solid #d1ecf1;
            border-radius: 6px;
            padding: 15px;
            font-family: 'SF Mono', Monaco, Inconsolata, 'Roboto Mono', Consolas, monospace;
            font-size: 0.85rem;
            margin: 20px 0;
        }

        footer {
            margin-top: 60px;
            padding-top: 30px;
            border-top: 1px solid #e1e4e8;
            text-align: center;
            color: #6a737d;
            font-size: 0.9rem;
        }

        @media (max-width: 768px) {
            h1 {
                font-size: 2rem;
            }
            
            .container {
                padding: 20px 15px;
            }
            
            .btn {
                display: block;
                margin: 8px 0;
                text-align: center;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1> Enhancing LLM Inference Efficiency with Key-Value Retrieval </h1>
            <div class="authors">Jihwan Kwak, Jungyoon Hwang, Sunghwan Joo, Sangwon Jung, and Taesup Moon</div>
            <div class="affiliation">M.IN.D Lab, Department of ECE, Seoul National University</div>
            
<!--             <div class="links">
                <a href="#" class="btn">ðŸ“„ Paper</a>
                <a href="#" class="btn btn-secondary">ðŸ’» Code</a>
                <a href="#" class="btn btn-secondary">ðŸ“Š Dataset</a>
                <a href="#" class="btn btn-secondary">ðŸŽ¥ Presentation</a>
            </div> -->
        </header>

<!--         <section>
            <h2>Abstract</h2>
            <div class="abstract">
                <p>Domain-Adaptive Pre-training (DAP) is useful because we can gain a lot of performance improvement by post-pretraining the foundation model. Against the naive fine-tuning, where directly fine-tune the foundation model to the target task, DAP, where fine-tuning is conducted after pre-training of foundation model on the task-related domain, shows dramatically high performance.</p>
                
                <p>Recently, continual DAP setting has been focused on since there are several domains to learn. The goal of continual DAP is to build a post-trained foundation model that has cumulative knowledge on seen domains so that it can be effectively fine-tuned on any tasks.</p>
            </div>
        </section> -->

        <section>
            <h2>Background</h2>
            <p>The KV cache mechanism, while essential for avoiding redundant computations during autoregressive generation, presents a critical bottleneck in LLM inference efficiency. As sequences extend, the memory footprint of stored key-value pairs grows linearly with sequence length and can quickly exceed available GPU memory, forcing expensive memory transfers or limiting the maximum context length. Furthermore, the attention computation over all cached key-value pairs remains computationally intensive, even when many historical tokens may have minimal relevance to current predictions.</p>
            
            <p>Recently, KV eviction strategies where eliminating redundant key-values before decoding phase have shown dramatic inference gains, reducing both KV memories and response latency. However, this hard eviction strategies are only restricted to single-turn QA tasks and have shown to not work for tasks that require long generation such as summarization or multi-turn conversations. The need for more intelligent and dynamic approaches to KV cache management has become increasingly apparent as LLMs are deployed in resource-constrained environments and real-time applications.</p>
        </section>

        <section>
            <h2>Objective</h2>
            <p> The primary goal of this research is to develop an efficient key-value retrieval framework that significantly reduces the computational overhead of LLM inference while preserving generation quality. Our approach aims to intelligently identify and retrieve only the most relevant key-value pairs from the cache based on the current generation context, rather than maintaining and processing the entire historical cache. </p>
        </section>

         <section>
            <h2>Method</h2>
            <h3>Motivation</h3>
            <p> To assess the potential for improvement beyond previous works that achieved at most 20% KV retention, we conducted experiments with an "upperbound" model that leverages ground-truth attention scores. This upperbound model retrieves the top-k key-value pairs based on the complete attention scores from each attention head. Implementation details include</p>
            <ul>
                <li> Dataset: HotpotQA, Musique, Qasper, 2wikimqa (Knowledge QA task in LongBench Benchmark)</li>
                <li> Model: Qwen2-7B-Instruct</li>
                <li> X-axis: KV budget (mean comp ratio) during decoding </li>
                <li> Y-axis : F1 score </li>
            </ul>
             <p> The results demonstrate that the upperbound model maintains performance comparable to full KV usage while utilizing only 4 to 16 key-value pairsâ€”less than 0.2% of the total KV cache. These findings indicate significant room for improvement in current KV retrieval methods. </p>

            <div class="figures-grid">
                <div class="figure-item">
                    <img src="./hotpotqa.png" alt="Results on HotpotQA dataset">
                    <div class="figure-caption">Figure 1: Results on HotpotQA</div>
                </div>
                <div class="figure-item">
                    <img src="./musique.png" alt="Results on Musique dataset">
                    <div class="figure-caption">Figure 2: Results on Musique</div>
                </div>
                <div class="figure-item">
                    <img src="./qasper.png" alt="Results on Qasper dataset">
                    <div class="figure-caption">Figure 3: Results on Qasper</div>
                </div>
                <div class="figure-item">
                    <img src="./2wikimqa.png" alt="Results on 2wikimqa dataset">
                    <div class="figure-caption">Figure 4: Results on 2wikimqa</div>
                </div>
            </div>
        </section>

  <!--      <section>
            <h2>Experiments</h2>
            <h3>Datasets</h3>
            <p>We evaluate our method on multiple domain adaptation benchmarks including:</p>
            <ul>
                <li>Domain-specific text corpora</li>
                <li>Multi-domain classification tasks</li>
                <li>Cross-domain transfer learning scenarios</li>
            </ul>

            <h3>Results</h3>
            <div class="results-grid">
                <div class="result-card">
                    <div class="metric">85.2%</div>
                    <div class="metric-label">Average Accuracy</div>
                </div>
                <div class="result-card">
                    <div class="metric">12.3%</div>
                    <div class="metric-label">Improvement over Baseline</div>
                </div>
                <div class="result-card">
                    <div class="metric">3.1%</div>
                    <div class="metric-label">Forgetting Rate</div>
                </div>
            </div>

            <div class="figure">
                <img src="https://via.placeholder.com/600x400/f6f8fa/6a737d?text=Results+Comparison+Chart" alt="Results">
                <div class="figure-caption">Figure 2: Performance comparison across different domains and methods</div>
            </div>
        </section>

        <section>
            <h2>Code Example</h2>
            <div class="code-block">
# Example usage of our continual learning framework
from continual_lora import ContinualLoRAModel

model = ContinualLoRAModel(
    base_model="bert-base-uncased",
    lora_rank=16,
    domains=["medical", "legal", "scientific"]
)

# Train on each domain incrementally
for domain in domains:
    model.train_domain(domain_data[domain])
    model.consolidate_knowledge()

# Fine-tune on downstream task
model.fine_tune(target_task_data)
            </div>
        </section>

        <section>
            <h2>Citation</h2>
            <div class="citation">
@inproceedings{kim2024continual,
  title={Continual Domain Incremental Learning on LoRA},
  author={Kim, Dohoon and Moon, Taesup},
  booktitle={Proceedings of [Conference Name]},
  year={2024},
  organization={[Publisher]}
}
            </div>
        </section> -->

        <footer>
            <p>Â© 2025 M.IN.D Lab, Seoul National University. All rights reserved.</p>
            <p>For questions, please contact: [kkwakzi@snu.ac.kr]</p>
        </footer>
    </div>
</body>
